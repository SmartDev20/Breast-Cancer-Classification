{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bcf4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "INPUT_DATASET = 'Datasets/Original'\n",
    "\n",
    "BASE_PATH = \"Datasets/IDC\"\n",
    "\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH , \"training\"])\n",
    "\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH , \"testing\"])\n",
    "\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH , \"validation\"])\n",
    "\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "VAL_SPLIT = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b31a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set\n",
      "Building directory Datasets/IDC/training\n",
      "Building directory Datasets/IDC/training/1\n",
      "Building directory Datasets/IDC/training/0\n",
      "Building validation set\n",
      "Building directory Datasets/IDC/validation\n",
      "Building directory Datasets/IDC/validation/0\n",
      "Building directory Datasets/IDC/validation/1\n",
      "Building testing set\n",
      "Building directory Datasets/IDC/testing\n",
      "Building directory Datasets/IDC/testing/0\n",
      "Building directory Datasets/IDC/testing/1\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import random , shutil , os\n",
    "from imutils import paths\n",
    "\n",
    "\n",
    "originalPaths = list(paths.list_images(config.INPUT_DATASET))\n",
    "random.seed(7)\n",
    "random.shuffle(originalPaths)\n",
    "\n",
    "index = int(len(originalPaths) * config.TRAIN_SPLIT)\n",
    "trainPaths = originalPaths[:index]\n",
    "testPaths = originalPaths[index:]\n",
    "\n",
    "\n",
    "index = int(len(trainPaths) * config.VAL_SPLIT)\n",
    "valPaths = trainPaths[:index]\n",
    "trainPaths = trainPaths[index:]\n",
    "\n",
    "datasets = (['training' , trainPaths , config.TRAIN_PATH] , \n",
    "            ['validation' , valPaths , config.VAL_PATH] , \n",
    "            ['testing' , testPaths , config.TEST_PATH])\n",
    "\n",
    "for (setType , originalPaths , basePath) in datasets :\n",
    "    print(f'Building {setType} set')\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(basePath) :\n",
    "        print(f'Building directory {basePath}')\n",
    "        os.makedirs(basePath)\n",
    "        \n",
    "        \n",
    "    for path in originalPaths :\n",
    "        file = path.split(os.path.sep)[-1]\n",
    "        label = file[-5:-4]\n",
    "        \n",
    "        labelPath = os.path.sep.join([basePath , label])\n",
    "        if not os.path.exists(labelPath) :\n",
    "            print(f'Building directory {labelPath}')\n",
    "            os.makedirs(labelPath)\n",
    "            \n",
    "            \n",
    "        newPath = os.path.sep.join([labelPath , file])\n",
    "        shutil.copy2(path , newPath)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc2d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import layer_normalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import SeparableConv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as k\n",
    "\n",
    "\n",
    "class CancerNet :\n",
    "    @staticmethod\n",
    "    def build(width , height , depth , classes) :\n",
    "        model = Sequential()\n",
    "        shape = (height , width , depth)\n",
    "        channelDim = -1\n",
    "        \n",
    "        if k.image_data_format == 'channels_first' :\n",
    "            shape = (depth , height , width)\n",
    "            channelDim = 1\n",
    "        \n",
    "        model.add(SeparableConv2D(32 , (3,3) , padding='same' , input_shape=shape))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(axis=channelDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(SeparableConv2D(64 , (3,3) , padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(axis=channelDim))\n",
    "        \n",
    "        model.add(SeparableConv2D(64 , (3,3) , padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(axis=channelDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(SeparableConv2D(128 , (3,3) , padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(axis=channelDim))\n",
    "        \n",
    "        \n",
    "        model.add(SeparableConv2D(128 , (3,3) , padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(axis=channelDim))\n",
    "        \n",
    "        model.add(SeparableConv2D(128 , (3,3) , padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(axis=channelDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a520b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 255750 images belonging to 2 classes.\n",
      "Found 42601 images belonging to 2 classes.\n",
      "Found 99926 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nabih/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5101/645600661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m M=model.fit(\n\u001b[0m\u001b[1;32m     85\u001b[0m   \u001b[0mtrainGen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlenTrain\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m     self._configure_dataset_and_inferred_steps(strategy, x, steps_per_epoch,\n\u001b[0m\u001b[1;32m   1159\u001b[0m                                                class_weight, distribute)\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_configure_dataset_and_inferred_steps\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import adagrad_v2\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from cancernet import CancerNet\n",
    "import config\n",
    "\n",
    "\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 40\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "trainPaths = list(paths.list_images(config.TRAIN_PATH))\n",
    "lenTrain = len(trainPaths)\n",
    "lenVal = len((list(paths.list_images(config.VAL_PATH))))\n",
    "lenTest = len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "trainLabels=[int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
    "trainLabels=np_utils.to_categorical(trainLabels)\n",
    "classTotals=trainLabels.sum(axis=0)\n",
    "classWeight=classTotals.max()/classTotals\n",
    "\n",
    "trainAug = ImageDataGenerator(\n",
    "           rescale=1/255.0 ,\n",
    "           rotation_range=20 ,\n",
    "           zoom_range=0.05 ,\n",
    "           width_shift_range=0.1 ,\n",
    "           height_shift_range=0.1 ,\n",
    "           shear_range=0.05 ,\n",
    "           vertical_flip=True ,\n",
    "           horizontal_flip=True ,\n",
    "           fill_mode='nearest')\n",
    "\n",
    "valAug = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "                                         config.TRAIN_PATH ,\n",
    "                                         class_mode='categorical' ,\n",
    "                                         target_size=(48,48) ,\n",
    "                                         color_mode='rgb' , \n",
    "                                         shuffle=True ,\n",
    "                                          batch_size=BS)\n",
    "\n",
    "\n",
    "valGen = trainAug.flow_from_directory(\n",
    "                                         config.VAL_PATH ,\n",
    "                                         class_mode='categorical' ,\n",
    "                                         target_size=(48,48) ,\n",
    "                                         color_mode='rgb' , \n",
    "                                         shuffle=False ,\n",
    "                                          batch_size=BS)\n",
    "\n",
    "\n",
    "testGen = trainAug.flow_from_directory(\n",
    "                                         config.TEST_PATH ,\n",
    "                                         class_mode='categorical' ,\n",
    "                                         target_size=(48,48) ,\n",
    "                                         color_mode='rgb' , \n",
    "                                         shuffle=False ,\n",
    "                                          batch_size=BS)\n",
    "\n",
    "\n",
    "model = CancerNet.build(width=48 , height=48 , depth=3 , classes=2)\n",
    "opt = Adagrad(lr=INIT_LR , decay = INIT_LR/NUM_EPOCHS)\n",
    "model.compile(loss='binary_crossentropy' , optimizer=opt , metrics=['accuracy'])\n",
    "\n",
    "M=model.fit(\n",
    "  trainGen,\n",
    "  steps_per_epoch=lenTrain//BS,\n",
    "  validation_data=valGen,\n",
    "  validation_steps=lenVal//BS,\n",
    "  class_weight=classWeight,\n",
    "  epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "print(\"Now evaluating the model\")\n",
    "testGen.reset()\n",
    "pred_indices = model.predict_generator(testGen , steps=(lenTest//BS)+1)\n",
    "pred_indices = np.argmax(pred_indices , axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(\n",
    "      testGen.classes ,\n",
    "      pred_indices , \n",
    "      target_names=testGen.class_indices.keys()))\n",
    "\n",
    "cm = confusion_matrix(testGen.classes , pred_indices)\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "specificity=cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "\n",
    "print(cm)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,N), M.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on the IDC Dataset\")\n",
    "plt.xlabel(\"Epoch No.\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1660363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 255750 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trainGen = trainAug.flow_from_directory(\n",
    "                                         config.TRAIN_PATH ,\n",
    "                                         class_mode='categorical' ,\n",
    "                                         target_size=(48,48) ,\n",
    "                                         color_mode='rgb' , \n",
    "                                         shuffle=True ,\n",
    "                                          batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e66f88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.preprocessing.image.DirectoryIterator"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd33a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42601 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valGen = trainAug.flow_from_directory(\n",
    "                                         config.VAL_PATH ,\n",
    "                                         class_mode='categorical' ,\n",
    "                                         target_size=(48,48) ,\n",
    "                                         color_mode='rgb' , \n",
    "                                         shuffle=False ,\n",
    "                                          batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066cfb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPaths=list(paths.list_images(config.TRAIN_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a3f441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255750"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "593a877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183200.  72550.]\n",
      "[1.       2.525155]\n"
     ]
    }
   ],
   "source": [
    "trainLabels=[int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
    "trainLabels=np_utils.to_categorical(trainLabels)\n",
    "classTotals=trainLabels.sum(axis=0)\n",
    "classWeight=classTotals.max()/classTotals\n",
    "print(classTotals)\n",
    "print(classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d07affb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255750"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfc836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 255750 images belonging to 2 classes.\n",
      "Found 42601 images belonging to 2 classes.\n",
      "Found 99926 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-25 22:33:44.275885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-25 22:33:44.275929: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-25 22:33:44.275958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ews): /proc/driver/nvidia/version does not exist\n",
      "2021-09-25 22:33:44.277155: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/nabih/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "/home/nabih/.local/lib/python3.8/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12302/633820216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m M=model.fit_generator(\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mtrainGen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlenTrain\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1973\u001b[0m                   \u001b[0;34m'will be removed in a future version. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[0;32m-> 1975\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   1976\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m     self._configure_dataset_and_inferred_steps(strategy, x, steps_per_epoch,\n\u001b[0m\u001b[1;32m   1159\u001b[0m                                                class_weight, distribute)\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_configure_dataset_and_inferred_steps\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from cancernet import CancerNet\n",
    "import config\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NUM_EPOCHS=40; INIT_LR=1e-2; BS=32\n",
    "\n",
    "trainPaths=list(paths.list_images(config.TRAIN_PATH))\n",
    "lenTrain=len(trainPaths)\n",
    "lenVal=len(list(paths.list_images(config.VAL_PATH)))\n",
    "lenTest=len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "trainLabels=[int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
    "trainLabels=np_utils.to_categorical(trainLabels)\n",
    "classTotals=trainLabels.sum(axis=0)\n",
    "classWeight=classTotals.max()/classTotals\n",
    "\n",
    "trainAug = ImageDataGenerator(\n",
    "\trescale=1/255.0,\n",
    "\trotation_range=20,\n",
    "\tzoom_range=0.05,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.05,\n",
    "\thorizontal_flip=True,\n",
    "\tvertical_flip=True,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "valAug=ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "\tconfig.TRAIN_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(48,48),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=True,\n",
    "\tbatch_size=BS)\n",
    "valGen = valAug.flow_from_directory(\n",
    "\tconfig.VAL_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(48,48),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False,\n",
    "\tbatch_size=BS)\n",
    "testGen = valAug.flow_from_directory(\n",
    "\tconfig.TEST_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(48,48),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False,\n",
    "\tbatch_size=BS)\n",
    "\n",
    "model=CancerNet.build(width=48,height=48,depth=3,classes=2)\n",
    "opt=Adagrad(lr=INIT_LR,decay=INIT_LR/NUM_EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "M=model.fit_generator(\n",
    "\ttrainGen,\n",
    "\tsteps_per_epoch=lenTrain//BS,\n",
    "\tvalidation_data=valGen,\n",
    "\tvalidation_steps=lenVal//BS,\n",
    "\tclass_weight=classWeight,\n",
    "\tepochs=NUM_EPOCHS)\n",
    "\n",
    "print(\"Now evaluating the model\")\n",
    "testGen.reset()\n",
    "pred_indices=model.predict_generator(testGen,steps=(lenTest//BS)+1)\n",
    "\n",
    "pred_indices=np.argmax(pred_indices,axis=1)\n",
    "\n",
    "print(classification_report(testGen.classes, pred_indices, target_names=testGen.class_indices.keys()))\n",
    "\n",
    "cm=confusion_matrix(testGen.classes,pred_indices)\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "specificity=cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print(cm)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,N), M.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on the IDC Dataset\")\n",
    "plt.xlabel(\"Epoch No.\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e6a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
